{"cells":[{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T16:50:28.772900Z","iopub.status.busy":"2023-11-13T16:50:28.772514Z","iopub.status.idle":"2023-11-13T16:50:43.604811Z","shell.execute_reply":"2023-11-13T16:50:43.603822Z","shell.execute_reply.started":"2023-11-13T16:50:28.772868Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy==1.22.4 in c:\\users\\atuli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.22.4)\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.3 -> 23.3.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["%pip install numpy==1.22.4"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:51:59.678557Z","iopub.status.busy":"2023-11-13T17:51:59.678200Z","iopub.status.idle":"2023-11-13T17:51:59.688582Z","shell.execute_reply":"2023-11-13T17:51:59.687452Z","shell.execute_reply.started":"2023-11-13T17:51:59.678528Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<contextlib.ExitStack at 0x1d1cf66ca50>"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import torch\n","import pandas as pd\n","from skimage import io, transform\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","plt.ion()   # interactive mode"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preparation\n","- Create Dataloader class\n","\n","Note: Working on Part (a) as of now.  \n","Guiding light: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"]},{"cell_type":"code","execution_count":112,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:34:51.523866Z","iopub.status.busy":"2023-11-13T17:34:51.523123Z","iopub.status.idle":"2023-11-13T17:34:51.532580Z","shell.execute_reply":"2023-11-13T17:34:51.531555Z","shell.execute_reply.started":"2023-11-13T17:34:51.523832Z"},"trusted":true},"outputs":[],"source":["START_TOKEN = \"START\"\n","END_TOKEN = \"END\"\n","UNK_TOKEN = \"UNK\"\n","\n","class Vocabulary:\n","    def __init__(self, freq_dict, wd_to_id, id_to_wd):\n","        self.freq_dict = freq_dict\n","        self.wd_to_id = wd_to_id\n","        self.id_to_wd = id_to_wd\n","        self.N = len(freq_dict)\n","    \n","    def get_id(self, word):\n","        if word in self.wd_to_id:\n","            return self.wd_to_id[word]\n","        else:\n","            return self.wd_to_id[UNK_TOKEN]\n","\n","class LatexFormulaDataset(Dataset):\n","    \"\"\"Latex Formula Dataset: Image and Text\"\"\"\n","    \n","    def __init__(self, csv_file, root_dir, transform = None):\n","        \"\"\"\n","        Arguments:\n","            csv_file (string): Path to the csv file with image name and text\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        #@TODO: May want to preload images\n","        self.df = pd.read_csv(csv_file)\n","        \n","        print('Debuggin:-----------------')\n","        print(self.df.head())\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","        '''Tokenize the formula by splitting on spaces'''\n","        self.df['formula'] = self.df['formula'].apply(lambda x: x.split())\n","        self.vocab= self.construct_vocab()  \n","\n","        self.maxlen = 0\n","        for formula in self.df['formula']:\n","            if len(formula) > self.maxlen:\n","                self.maxlen = len(formula)\n","\n","        self.df['formula'] = self.df['formula'].apply(lambda x: x + [UNK_TOKEN]*(self.maxlen - len(x)))\n","\n","        #Embedding layer\n","        self.embed = nn.Embedding(self.vocab.N, 512)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns sample of type image, textformula\n","        \"\"\"\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir,\n","                                self.df.iloc[idx, 0])\n","        image = io.imread(img_name)\n","        formula = self.df.iloc[idx, 1]\n","        formula = np.array([formula], dtype=str).reshape(-1, 1)\n","        formula = [[self.vocab.get_id(wd[0]) for wd in formula]] \n","        sample = {'image': image, 'formula': torch.tensor(formula, dtype=torch.int64)}\n","\n","        if self.transform:\n","            sample['image'] = self.transform(sample['image'])\n","            \n","        return sample \n","    \n","    def construct_vocab(self):\n","        \"\"\"\n","        Constructs vocabulary from the dataset formulas\n","        \"\"\"\n","        freq_dict = {}\n","        for formula in self.df['formula']:\n","            for wd in formula:\n","                if wd not in freq_dict:\n","                    freq_dict[wd] = 1\n","                else:\n","                    freq_dict[wd] += 1\n","        freq_dict[START_TOKEN] = 1\n","        freq_dict[END_TOKEN] = 1\n","        freq_dict[UNK_TOKEN] = 1\n","        N = len(freq_dict)\n","        wd_to_id = {}\n","        for i, wd in enumerate(freq_dict):\n","            wd_to_id[wd] = i\n","        id_to_wd = {v: k for k, v in wd_to_id.items()}\n","    \n","        #pad the formulas with \n","        return Vocabulary(freq_dict, wd_to_id, id_to_wd)      \n","\n","def get_dataloader(csv_path, image_root, batch_size, transform = None):\n","    \"\"\"\n","    Returns dataloader for the dataset\n","    \"\"\"\n","    dataset = LatexFormulaDataset(csv_path, image_root, transform) #checked\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    return dataloader, dataset\n","     "]},{"cell_type":"markdown","metadata":{},"source":["### Encoder Network\n","- A CNN to encode image to more meaningful vector"]},{"cell_type":"code","execution_count":142,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T18:15:50.390411Z","iopub.status.busy":"2023-11-13T18:15:50.390048Z","iopub.status.idle":"2023-11-13T18:15:50.401998Z","shell.execute_reply":"2023-11-13T18:15:50.401089Z","shell.execute_reply.started":"2023-11-13T18:15:50.390380Z"},"trusted":true},"outputs":[],"source":["class EncoderCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","    \n","        #@TODO:reduce number of layers: eliminate pools and acts\n","        self.conv1 = nn.Conv2d(3, 32, (5, 5))\n","        self.act1 = nn.ReLU()\n","        self.pool1 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv2 = nn.Conv2d(32, 64, (5, 5))\n","        self.act2 = nn.ReLU()\n","        self.pool2 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv3 = nn.Conv2d(64, 128, (5, 5))\n","        self.act3 = nn.ReLU()\n","        self.pool3 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv4 = nn.Conv2d(128, 256, (5, 5))\n","        self.act4 = nn.ReLU()\n","        self.pool4 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv5 = nn.Conv2d(256, 512, (5, 5))\n","        self.act5 = nn.ReLU()\n","        self.pool5 = nn.MaxPool2d((2, 2))\n","        \n","        self.avg_pool = nn.AvgPool2d((3, 3))\n","    \n","    def forward(self, x):\n","        x = self.act1(self.conv1(x))\n","        x = self.pool1(x)\n","        \n","        x = self.act2(self.conv2(x))\n","        x = self.pool2(x)\n","        \n","        x = self.act3(self.conv3(x))\n","        x = self.pool3(x)\n","        \n","        x = self.act4(self.conv4(x))\n","        x = self.pool4(x)\n","        \n","        x = self.act5(self.conv5(x))\n","        x = self.pool5(x)\n","        \n","        x = self.avg_pool(x)\n","        x = x.view(x.size(0), -1, 512) \n","        return x"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-11-13T18:15:56.106481Z","iopub.status.busy":"2023-11-13T18:15:56.105734Z","iopub.status.idle":"2023-11-13T18:15:56.198416Z","shell.execute_reply":"2023-11-13T18:15:56.197693Z","shell.execute_reply.started":"2023-11-13T18:15:56.106446Z"}},"source":["### Vocabulary\n","- https://github.com/harvardnlp/im2markup/blob/master"]},{"cell_type":"markdown","metadata":{},"source":["### Decoder Network"]},{"cell_type":"code","execution_count":206,"metadata":{},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    \"\"\"\n","    INPUTS\n","    context_size : size of the context vector\n","    hidden_size : size of the hidden latent vectors\n","    embed_size : literal\n","    vocab_size : literal\n","    output_size : one_hot?\n","    \"\"\"\n","    def __init__(self, vocab, context_size, hidden_size, embed_size, output_size, max_length):\n","        super().__init__()\n","\n","        #class variables\n","        self.embed_size = embed_size\n","        self.context_size = context_size\n","        self.max_length = max_length\n","        self.vocab = vocab\n","        vocab_size = vocab.N\n","\n","        #compute input size, concatenating context and prev. output embedding\n","        input_size = context_size + embed_size\n","\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers = 1)\n","\n","        self.out = nn.Linear(hidden_size, output_size) #output_size = vocab_size\n","    \n","    def forward(self, context, target_tensor = None):\n","        \"\"\"\n","        target_tensor is of size MAX_LENGTH\n","        \"\"\"\n","        #START Token handling\n","        batch_size = context.size(0)\n","        start_id = self.vocab.get_id(START_TOKEN)\n","        start_tensor = torch.empty(batch_size, 1, dtype = torch.int64).fill_(start_id)\n","\n","        decoder_input = torch.concatenate((context, self.embedding(start_tensor)), dim = 0)\n","\n","        print(f'Context shape: {context.shape}, decoder_input shape: {decoder_input.shape}')\n","        print(f'embedding shape: {self.embedding(start_tensor).shape}')\n","        print('====================================')\n","\n","        decoder_hidden = context  #dimensions are same\n","        decoder_outputs = []\n","\n","        for i in range(self.max_length):\n","            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n","            decoder_outputs.append(decoder_output)\n","\n","            if target_tensor is not None:\n","                input_tensor = self.vocab.get_id(target_tensor[i])  #assuming target_tensor[i] is just a number\n","                ground_truth_embed = self.embedding(input_tensor)\n","                decoder_input = torch.concatenate((context, ground_truth_embed), dim = 0)\n","            else:\n","                #embed the last output, which was an index of vocab\n","                last_out_embed = self.embedding(decoder_outputs[-1])\n","                decoder_input = torch.concatenate((context, last_out_embed), dim = 0)\n","\n","        return decoder_outputs, decoder_hidden, None\n","        \n","    def forward_step(self, input, hidden):\n","        print('+++++++++++++++++++++++++=')\n","        print(f'Input shape: {input.shape}, hidden shape: {hidden.shape}')\n","        output, hidden = self.lstm(input, hidden)\n","        print(f'New hidden shape: {hidden.shape}')\n","        output = self.out(hidden)\n","\n","        #get the output as just an index tensor\n","        output = torch.argmax(output, dim = -1)\n","\n","        return output, hidden"]},{"cell_type":"markdown","metadata":{},"source":["### Training Code.\n","- Dataloader automatically loads in batches. The data need not be modified by us."]},{"cell_type":"code","execution_count":207,"metadata":{},"outputs":[],"source":["def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n","    total_loss = 0\n","    for data in dataloader:\n","        input_tensor, target_tensor = data['image'], data['formula']\n","\n","        encoder_optimizer.zero_grad()\n","        decoder_optimizer.zero_grad()\n","\n","        encoder_output = encoder(input_tensor)\n","        decoder_outputs, _, _ = decoder(encoder_output)\n","\n","        loss = criterion(\n","            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n","            target_tensor.view(-1)\n","        )\n","        loss.backward()\n","\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / len(dataloader)"]},{"cell_type":"code","execution_count":208,"metadata":{},"outputs":[],"source":["import time\n","import math\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"]},{"cell_type":"code","execution_count":209,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.switch_backend('agg')\n","import matplotlib.ticker as ticker\n","import numpy as np\n","\n","def showPlot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    # this locator puts ticks at regular intervals\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)"]},{"cell_type":"code","execution_count":210,"metadata":{},"outputs":[],"source":["def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=100, plot_every=100):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n","    criterion = nn.CrossEntropyLoss() #as stated in assignment\n","\n","    for epoch in range(1, n_epochs + 1):\n","        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        if epoch % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n","                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n","\n","        if epoch % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","    showPlot(plot_losses)"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":211,"metadata":{},"outputs":[],"source":["batch_size = 32\n","vocab_size = 1000\n","CONTEXT_SIZE = 512\n","HIDDEN_SIZE = 512\n","OUTPUT_SIZE  = vocab_size\n","MAX_LENGTH = 10000"]},{"cell_type":"code","execution_count":212,"metadata":{},"outputs":[],"source":["# image processing\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((224, 224)),\n","    transforms.Lambda(lambda x: x/255.0), #min-max normalisation\n","])"]},{"cell_type":"code","execution_count":213,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Debuggin:-----------------\n","            image                                            formula\n","0  74d337e8a0.png  $ \\gamma _ { \\Omega R , 5 } ^ { T } = - \\gamma...\n","1  2d0f18f71d.png  $ l ^ { ( -- ) \\underline { { m } } } u _ { \\u...\n","2  6d9b9de88d.png  $ \\left[ H , \\gamma _ { i } ^ { \\left( 2 \\righ...\n","3  38c6d510bb.png  $ < a _ { i } > \\; \\propto \\; \\int _ { \\omega ...\n","4  24537a86e3.png  $ \\Psi ( \\mu _ { 1 } , \\ldots , \\mu _ { K } ) ...\n","(75000, 2)\n"]}],"source":["#part a\n","#train_csv_path = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/train.csv\"\n","#image_root_path = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/images\"\n","train_csv_path = \"data/SyntheticData/train.csv\"\n","image_root_path = \"data/SyntheticData/images\"\n","train_dataloader, train_dataset = get_dataloader(train_csv_path, image_root_path, batch_size, transform)\n","vocab = train_dataset.vocab\n","MAX_LENGTH = train_dataset.maxlen\n","\n","print(train_dataset.df.shape)"]},{"cell_type":"code","execution_count":214,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Context shape: torch.Size([32, 1, 512]), decoder_input shape: torch.Size([64, 1, 512])\n","embedding shape: torch.Size([32, 1, 512])\n","====================================\n","+++++++++++++++++++++++++=\n","Input shape: torch.Size([64, 1, 512]), hidden shape: torch.Size([32, 1, 512])\n"]},{"ename":"RuntimeError","evalue":"For batched 3-D input, hx and cx should also be 3-D but got (2-D, 2-D) tensors","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32mc:\\Users\\atuli\\OneDrive - IIT Delhi\\Desktop\\IIT Sem5\\COL774\\Assignments\\A4\\Handwriting-to-Latex-ML\\a4-q1 tool.ipynb Cell 19\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m encoder \u001b[39m=\u001b[39m EncoderCNN()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m decoder \u001b[39m=\u001b[39m DecoderRNN(vocab, CONTEXT_SIZE, HIDDEN_SIZE, \u001b[39m512\u001b[39m, OUTPUT_SIZE, MAX_LENGTH)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train(train_dataloader, encoder, decoder, \u001b[39m10\u001b[39;49m)\n","\u001b[1;32mc:\\Users\\atuli\\OneDrive - IIT Delhi\\Desktop\\IIT Sem5\\COL774\\Assignments\\A4\\Handwriting-to-Latex-ML\\a4-q1 tool.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss() \u001b[39m#as stated in assignment\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     loss \u001b[39m=\u001b[39m train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     print_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     plot_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n","\u001b[1;32mc:\\Users\\atuli\\OneDrive - IIT Delhi\\Desktop\\IIT Sem5\\COL774\\Assignments\\A4\\Handwriting-to-Latex-ML\\a4-q1 tool.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m decoder_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m encoder_output \u001b[39m=\u001b[39m encoder(input_tensor)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m decoder_outputs, _, _ \u001b[39m=\u001b[39m decoder(encoder_output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     decoder_outputs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, decoder_outputs\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     target_tensor\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n","File \u001b[1;32mc:\\Users\\atuli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[1;32mc:\\Users\\atuli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","\u001b[1;32mc:\\Users\\atuli\\OneDrive - IIT Delhi\\Desktop\\IIT Sem5\\COL774\\Assignments\\A4\\Handwriting-to-Latex-ML\\a4-q1 tool.ipynb Cell 19\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m decoder_outputs \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     decoder_output, decoder_hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_step(decoder_input, decoder_hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     decoder_outputs\u001b[39m.\u001b[39mappend(decoder_output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mif\u001b[39;00m target_tensor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[1;32mc:\\Users\\atuli\\OneDrive - IIT Delhi\\Desktop\\IIT Sem5\\COL774\\Assignments\\A4\\Handwriting-to-Latex-ML\\a4-q1 tool.ipynb Cell 19\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m+++++++++++++++++++++++++=\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput shape: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, hidden shape: \u001b[39m\u001b[39m{\u001b[39;00mhidden\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atuli/OneDrive%20-%20IIT%20Delhi/Desktop/IIT%20Sem5/COL774/Assignments/A4/Handwriting-to-Latex-ML/a4-q1%20tool.ipynb#X51sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m#get the output as just an index tensor\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\atuli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[1;32mc:\\Users\\atuli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\atuli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:866\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    863\u001b[0m     \u001b[39mif\u001b[39;00m (hx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mor\u001b[39;00m hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m):\n\u001b[0;32m    864\u001b[0m         msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mFor batched 3-D input, hx and cx should \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39malso be 3-D but got (\u001b[39m\u001b[39m{\u001b[39;00mhx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D, \u001b[39m\u001b[39m{\u001b[39;00mhx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D) tensors\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 866\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m    867\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    868\u001b[0m     \u001b[39mif\u001b[39;00m hx[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m hx[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n","\u001b[1;31mRuntimeError\u001b[0m: For batched 3-D input, hx and cx should also be 3-D but got (2-D, 2-D) tensors"]}],"source":["#create a network instance\n","encoder = EncoderCNN()\n","decoder = DecoderRNN(vocab, CONTEXT_SIZE, HIDDEN_SIZE, 512, OUTPUT_SIZE, MAX_LENGTH)\n","train(train_dataloader, encoder, decoder, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
