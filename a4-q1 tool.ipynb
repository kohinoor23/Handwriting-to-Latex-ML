{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T16:50:28.772900Z","iopub.status.busy":"2023-11-13T16:50:28.772514Z","iopub.status.idle":"2023-11-13T16:50:43.604811Z","shell.execute_reply":"2023-11-13T16:50:43.603822Z","shell.execute_reply.started":"2023-11-13T16:50:28.772868Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy==1.22.4 in /Users/samarth/.pyenv/versions/3.10.4/lib/python3.10/site-packages (1.22.4)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install numpy==1.22.4"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:51:59.678557Z","iopub.status.busy":"2023-11-13T17:51:59.678200Z","iopub.status.idle":"2023-11-13T17:51:59.688582Z","shell.execute_reply":"2023-11-13T17:51:59.687452Z","shell.execute_reply.started":"2023-11-13T17:51:59.678528Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<contextlib.ExitStack at 0x291797e80>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import torch\n","import pandas as pd\n","from skimage import io, transform\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","plt.ion()   # interactive mode"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preparation\n","- Create Dataloader class\n","\n","Note: Working on Part (a) as of now."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:34:51.523866Z","iopub.status.busy":"2023-11-13T17:34:51.523123Z","iopub.status.idle":"2023-11-13T17:34:51.532580Z","shell.execute_reply":"2023-11-13T17:34:51.531555Z","shell.execute_reply.started":"2023-11-13T17:34:51.523832Z"},"trusted":true},"outputs":[],"source":["START_TOKEN = \"START\"\n","END_TOKEN = \"END\"\n","UNK_TOKEN = \"UNK\"\n","\n","class Vocabulary:\n","    def __init__(self, freq_dict, wd_to_id, id_to_wd):\n","        self.freq_dict = freq_dict\n","        self.wd_to_id = wd_to_id\n","        self.id_to_wd = id_to_wd\n","        self.N = len(freq_dict)\n","\n","class LatexFormulaDataset(Dataset):\n","    \"\"\"Latex Formula Dataset: Image and Text\"\"\"\n","    \n","    def __init__(self, csv_file, root_dir, transform = None):\n","        \"\"\"\n","        Arguments:\n","            csv_file (string): Path to the csv file with image name and text\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        #@TODO: May want to preload images\n","        self.df = pd.read_csv(csv_file)\n","        \n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","        '''Tokenize the formula by splitting on spaces'''\n","        self.df['formula'] = self.df['formula'].apply(lambda x: x.split())\n","        self.vocab= self.construct_vocab()  \n","\n","        maxlen = 0\n","        for formula in self.df['formula']:\n","            if len(formula) > maxlen:\n","                maxlen = len(formula)\n","\n","        self.df['formula'] = self.df['formula'].apply(lambda x: x + [UNK_TOKEN]*(maxlen - len(x)))\n","\n","    def __len__(self):\n","        return len(self.landmarks_frame)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns sample of type image, textformula\n","        \"\"\"\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir,\n","                                self.df.iloc[idx, 0])\n","        image = io.imread(img_name)\n","        formula = self.df.iloc[idx, 1]\n","        formula = np.array([formula], dtype=str).reshape(-1, 1)\n","        sample = {'image': image, 'formula': formula}\n","\n","        if self.transform:\n","            sample['image'] = self.transform(sample['image'])\n","\n","        return sample \n","    def construct_vocab(self):\n","        \"\"\"\n","        Constructs vocabulary from the dataset formulas\n","        \"\"\"\n","        freq_dict = {}\n","        for formula in self.df['formula']:\n","            for wd in formula:\n","                if wd not in freq_dict:\n","                    freq_dict[wd] = 1\n","                else:\n","                    freq_dict[wd] += 1\n","        freq_dict[START_TOKEN] = 1\n","        freq_dict[END_TOKEN] = 1\n","        freq_dict[UNK_TOKEN] = 1\n","        N = len(freq_dict)\n","        wd_to_id = {}\n","        for i, wd in enumerate(freq_dict):\n","            wd_to_id[wd] = i\n","        id_to_wd = {v: k for k, v in wd_to_id.items()}\n","    \n","        #pad the formulas with \n","        return Vocabulary(freq_dict, wd_to_id, id_to_wd)       "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:51:07.821400Z","iopub.status.busy":"2023-11-13T17:51:07.821028Z","iopub.status.idle":"2023-11-13T17:51:07.826658Z","shell.execute_reply":"2023-11-13T17:51:07.825542Z","shell.execute_reply.started":"2023-11-13T17:51:07.821371Z"},"trusted":true},"outputs":[],"source":["# image processing\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((224, 224)),\n","    transforms.Lambda(lambda x: x/255.0), #min-max normalisation\n","])"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:51:08.774305Z","iopub.status.busy":"2023-11-13T17:51:08.773240Z","iopub.status.idle":"2023-11-13T17:51:08.782964Z","shell.execute_reply":"2023-11-13T17:51:08.781920Z","shell.execute_reply.started":"2023-11-13T17:51:08.774267Z"},"trusted":true},"outputs":[],"source":["# image = io.imread('/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/images/100009e256.png')\n","# image = np.asarray(image)\n","# image.shape"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:51:12.844970Z","iopub.status.busy":"2023-11-13T17:51:12.844235Z","iopub.status.idle":"2023-11-13T17:51:13.020102Z","shell.execute_reply":"2023-11-13T17:51:13.019072Z","shell.execute_reply.started":"2023-11-13T17:51:12.844937Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["#part a\n","\n","# train_csv_path = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/train.csv\"\n","# image_root_path = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/images\"\n","# train_set = LatexFormulaDataset(train_csv_path, image_root_path, transform)\n","\n","# print(train_set[1]['image'])"]},{"cell_type":"markdown","metadata":{},"source":["### Encoder Network\n","- A CNN to encode image to more meaningful vector"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T18:15:50.390411Z","iopub.status.busy":"2023-11-13T18:15:50.390048Z","iopub.status.idle":"2023-11-13T18:15:50.401998Z","shell.execute_reply":"2023-11-13T18:15:50.401089Z","shell.execute_reply.started":"2023-11-13T18:15:50.390380Z"},"trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        self.conv1 = nn.Conv2d(3, 32, (5, 5))\n","        self.act1 = nn.ReLU()\n","        self.pool1 = nn.MaxPool2d((2,2))\n","        \n","        self.conv2 = nn.Conv2d(32, 64, (5, 5))\n","        self.act2 = nn.ReLU()\n","        self.pool2 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv3 = nn.Conv2d(64, 128, (5, 5))\n","        self.act3 = nn.ReLU()\n","        self.pool3 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv4 = nn.Conv2d(128, 256, (5, 5))\n","        self.act4 = nn.ReLU()\n","        self.pool4 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv5 = nn.Conv2d(256, 512, (5, 5))\n","        self.act5 = nn.ReLU()\n","        self.pool5 = nn.MaxPool2d((2, 2))\n","        \n","        self.avg_pool = nn.AvgPool2d((3, 3))\n","    \n","    def forward(self, x):\n","        x = self.act1(self.conv1(x))\n","        x = self.pool1(x)\n","        \n","        x = self.act2(self.conv2(x))\n","        x = self.pool2(x)\n","        \n","        x = self.act3(self.conv3(x))\n","        x = self.pool3(x)\n","        \n","        x = self.act4(self.conv4(x))\n","        x = self.pool4(x)\n","        \n","        x = self.act5(self.conv5(x))\n","        x = self.pool5(x)\n","        \n","        x = self.avg_pool(x)\n","        x = x.reshape((1, 1, 512))\n","        return x"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-11-13T18:15:56.106481Z","iopub.status.busy":"2023-11-13T18:15:56.105734Z","iopub.status.idle":"2023-11-13T18:15:56.198416Z","shell.execute_reply":"2023-11-13T18:15:56.197693Z","shell.execute_reply.started":"2023-11-13T18:15:56.106446Z"}},"source":["### Vocabulary\n","- https://github.com/harvardnlp/im2markup/blob/master"]},{"cell_type":"markdown","metadata":{},"source":["### Decoder Network"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["\n","\n","class Decoder(nn.Module):\n","    \"\"\"\n","    Inputs:\n","    (here M is whatever the batch size is passed)\n","\n","    context_size : size of the context vector [shape: (1,M,context_size)]\n","    n_layers: number of layers [for our purposes, defaults to 1]\n","    hidden_size : size of the hidden state vectors [shape: (n_layers,M,hidden_size)]\n","    embed_size : size of the embedding vectors [shape: (1,M,embed_size)]\n","    vocab_size : size of the vocabulary\n","    max_length : maximum length of the formula\n","    \"\"\"\n","    def __init__(self, context_size, vocab, n_layers = 1, hidden_size = 512, embed_size = 512,  max_length = 100):\n","        super().__init__()\n","        self.context_size = context_size\n","        self.vocab = vocab\n","        self.vocab_size = vocab.N\n","        self.n_layers = n_layers\n","        self.hidden_size = hidden_size\n","        self.embed_size = embed_size\n","        self.max_length = max_length\n","\n","\n","        self.input_size = context_size + embed_size\n","        self.embed = nn.Embedding(self.vocab_size, embed_size)\n","        \n","        self.lstm = nn.LSTM(self.input_size, hidden_size, n_layers)\n","        self.linear = nn.Linear(hidden_size, self.vocab_size)\n","        self.softmax = nn.Softmax(dim = 2)\n","\n","        pass\n","    \n","    def forward(self, context, target_tensor = None):\n","        \"\"\"\n","        context is the context vector from the encoder [shape: (1,M,context_size)]\n","        target_tensor is the formula in tensor form [shape: (1,M,max_length)] (in the second dimension, it is sequence of indices of formula tokens)\n","            if target_tensor is not None, then we are in Teacher Forcing mode\n","            else normal jo bhi (last prediction is concatenated)\n","        \"\"\"\n","        batch_size = context.shape[1]\n","\n","        #initialize hidden state and cell state\n","            #@TODO: Some caveat in the size of the cell vector. Should it be same as hidden_size? (check nn.LSTM documentation)\n","        hidden = torch.zeros((self.n_layers, batch_size, self.hidden_size))\n","        cell = torch.zeros((self.n_layers, batch_size, self.hidden_size))\n","\n","        #initialize the input with embedding of the start token\n","        init_embed = self.embed(torch.tensor([self.vocab.wd_to_id[START_TOKEN]])).reshape((1, batch_size, self.embed_size))\n","        input = torch.cat([context, init_embed], dim = 2)\n","\n","        #initialize the output\n","        output = torch.zeros((1, batch_size, self.vocab_size))\n","\n","        for i in range(self.max_length):\n","            output, (hidden, cell) = self.lstm(input, (hidden, cell))\n","            output = self.linear(output)\n","            output = self.softmax(output)\n","\n","            \n","            if target_tensor is not None:\n","                input = torch.cat([context, self.embed(target_tensor[0, :, i]).reshape((1,batch_size, self.embed_size))], dim = 2)\n","            else:\n","                #add the embedding of the last prediction\n","                input = torch.cat([context, self.embed(torch.argmax(output, dim = 2))], dim = 2)\n","        \n","            \n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["vocab_size = 1000\n","CONTEXT_SIZE = 512\n","HIDDEN_SIZE = 512\n","OUTPUT_SIZE  = vocab_size\n","MAX_LENGTH = 10000"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(3)\n"]}],"source":["print(torch.tensor(3))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
