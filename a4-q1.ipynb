{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T16:50:28.772900Z","iopub.status.busy":"2023-11-13T16:50:28.772514Z","iopub.status.idle":"2023-11-13T16:50:43.604811Z","shell.execute_reply":"2023-11-13T16:50:43.603822Z","shell.execute_reply.started":"2023-11-13T16:50:28.772868Z"},"trusted":true},"outputs":[],"source":["%pip install numpy==1.22.4"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:51:59.678557Z","iopub.status.busy":"2023-11-13T17:51:59.678200Z","iopub.status.idle":"2023-11-13T17:51:59.688582Z","shell.execute_reply":"2023-11-13T17:51:59.687452Z","shell.execute_reply.started":"2023-11-13T17:51:59.678528Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import pandas as pd\n","from skimage import io, transform\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","plt.ion()   # interactive mode"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preparation\n","- Create Dataloader class\n","\n","Note: Working on Part (a) as of now."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:34:51.523866Z","iopub.status.busy":"2023-11-13T17:34:51.523123Z","iopub.status.idle":"2023-11-13T17:34:51.532580Z","shell.execute_reply":"2023-11-13T17:34:51.531555Z","shell.execute_reply.started":"2023-11-13T17:34:51.523832Z"},"trusted":true},"outputs":[],"source":["class LatexFormulaDataset(Dataset):\n","    \"\"\"Latex Formula Dataset: Image and Text\"\"\"\n","    \n","    def __init__(self, csv_file, root_dir, transform = None):\n","        \"\"\"\n","        Arguments:\n","            csv_file (string): Path to the csv file with image name and text\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.df = pd.read_csv(csv_file)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.landmarks_frame)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns sample of type image, textformula\n","        \"\"\"\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir,\n","                                self.df.iloc[idx, 0])\n","        image = io.imread(img_name)\n","        formula = self.df.iloc[idx, 1]\n","        formula = np.array([formula], dtype=str).reshape(-1, 1)\n","        sample = {'image': image, 'formula': formula}\n","\n","        if self.transform:\n","            sample['image'] = self.transform(sample['image'])\n","\n","        return sample        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:51:07.821400Z","iopub.status.busy":"2023-11-13T17:51:07.821028Z","iopub.status.idle":"2023-11-13T17:51:07.826658Z","shell.execute_reply":"2023-11-13T17:51:07.825542Z","shell.execute_reply.started":"2023-11-13T17:51:07.821371Z"},"trusted":true},"outputs":[],"source":["# image processing\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((224, 224)),\n","    transforms.Lambda(lambda x: x/255.0), #min-max normalisation\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:51:08.774305Z","iopub.status.busy":"2023-11-13T17:51:08.773240Z","iopub.status.idle":"2023-11-13T17:51:08.782964Z","shell.execute_reply":"2023-11-13T17:51:08.781920Z","shell.execute_reply.started":"2023-11-13T17:51:08.774267Z"},"trusted":true},"outputs":[],"source":["image = io.imread('/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/images/100009e256.png')\n","image = np.asarray(image)\n","image.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:51:12.844970Z","iopub.status.busy":"2023-11-13T17:51:12.844235Z","iopub.status.idle":"2023-11-13T17:51:13.020102Z","shell.execute_reply":"2023-11-13T17:51:13.019072Z","shell.execute_reply.started":"2023-11-13T17:51:12.844937Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["#part a\n","train_csv_path = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/train.csv\"\n","image_root_path = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/images\"\n","train_set = LatexFormulaDataset(train_csv_path, image_root_path, transform)\n","\n","print(train_set[1]['image'])"]},{"cell_type":"markdown","metadata":{},"source":["### Encoder Network\n","- A CNN to encode image to more meaningful vector"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T18:15:50.390411Z","iopub.status.busy":"2023-11-13T18:15:50.390048Z","iopub.status.idle":"2023-11-13T18:15:50.401998Z","shell.execute_reply":"2023-11-13T18:15:50.401089Z","shell.execute_reply.started":"2023-11-13T18:15:50.390380Z"},"trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        self.conv1 = nn.Conv2d(3, 32, (5, 5))\n","        self.act1 = nn.ReLU()\n","        self.pool1 = nn.MaxPool2d((2,2))\n","        \n","        self.conv2 = nn.Conv2d(32, 64, (5, 5))\n","        self.act2 = nn.ReLU()\n","        self.pool2 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv3 = nn.Conv2d(64, 128, (5, 5))\n","        self.act3 = nn.ReLU()\n","        self.pool3 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv4 = nn.Conv2d(128, 256, (5, 5))\n","        self.act4 = nn.ReLU()\n","        self.pool4 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv5 = nn.Conv2d(256, 512, (5, 5))\n","        self.act5 = nn.ReLU()\n","        self.pool5 = nn.MaxPool2d((2, 2))\n","        \n","        self.avg_pool = nn.AvgPool2d((3, 3))\n","    \n","    def forward(self, x):\n","        x = self.act1(self.conv1(x))\n","        x = self.pool1(x)\n","        \n","        x = self.act2(self.conv2(x))\n","        x = self.pool2(x)\n","        \n","        x = self.act3(self.conv3(x))\n","        x = self.pool3(x)\n","        \n","        x = self.act4(self.conv4(x))\n","        x = self.pool4(x)\n","        \n","        x = self.act5(self.conv5(x))\n","        x = self.pool5(x)\n","        \n","        x = self.avg_pool(x)\n","        x = x.reshape((1, 1, 512))\n","        return x"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-11-13T18:15:56.106481Z","iopub.status.busy":"2023-11-13T18:15:56.105734Z","iopub.status.idle":"2023-11-13T18:15:56.198416Z","shell.execute_reply":"2023-11-13T18:15:56.197693Z","shell.execute_reply.started":"2023-11-13T18:15:56.106446Z"}},"source":["### Vocabulary\n","- https://github.com/harvardnlp/im2markup/blob/master"]},{"cell_type":"markdown","metadata":{},"source":["### Decoder Network"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    \"\"\"\n","    INPUTS\n","    context_size : size of the context vector\n","    hidden_size : size of the hidden latent vectors\n","    embed_size : literal\n","    vocab_size : literal\n","    output_size : one_hot?\n","    \"\"\"\n","    def __init__(self, context_size, hidden_size, embed_size, vocab_size, output_size, max_length):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","\n","        #class variables\n","        self.embed_size = embed_size\n","        self.context_size = context_size\n","        self.max_length = max_length\n","\n","        #compute input size: concatenated one\n","        input_size = context_size + embed_size\n","\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers = 1)\n","\n","        self.out = nn.Linear(hidden_size, output_size)\n","    \n","    def forward(self, context, target_tensor = None):\n","        \"\"\"\n","        target_tensor is of size MAX_LENGTH\n","        \"\"\"\n","\n","\n","        #SOS Token embedding?\n","        decoder_input = torch.empty(self.context_size+self.embed_size, 1, dtype=torch.long)\n","        decoder_hidden = context  #dimensions are same\n","        decoder_outputs = []\n","\n","        for i in range(self.max_length):\n","            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n","            decoder_outputs.append(decoder_output)\n","\n","            if target_tensor is not None:\n","\n","                #embed the target tensor : use vocab word_to_id?\n","                ground_truth_embed = self.embedding(target_tensor[i])\n","                decoder_input = torch.concatenate((context, ground_truth_embed), dim = 0)\n","            else:\n","                #embed the last timestep output: ?\n","                last_out_embed = self.embedding(decoder_outputs[-1])\n","                decoder_input = torch.concatenate((context, last_out_embed), dim = 0)\n","        \n","        #check if softmax is required before as well, should we add softmax in out layer itself?\n","        decoder_outputs = F.log_softmax(decoder_outputs, dim = -1)\n","        return decoder_outputs, decoder_hidden, None\n","        \n","    def forward_step(self, input, hidden):\n","        output, hidden = self.LSTM(input, hidden)\n","        output = self.out(hidden)\n","        return output, hidden\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["vocab_size = 1000\n","CONTEXT_SIZE = 512\n","HIDDEN_SIZE = 512\n","OUTPUT_SIZE  = vocab_size\n","MAX_LENGTH = 10000"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T16:50:28.772900Z","iopub.status.busy":"2023-11-13T16:50:28.772514Z","iopub.status.idle":"2023-11-13T16:50:43.604811Z","shell.execute_reply":"2023-11-13T16:50:43.603822Z","shell.execute_reply.started":"2023-11-13T16:50:28.772868Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting numpy==1.22.4\n","  Downloading numpy-1.22.4-cp310-cp310-macosx_11_0_arm64.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.24.0\n","    Uninstalling numpy-1.24.0:\n","      Successfully uninstalled numpy-1.24.0\n","Successfully installed numpy-1.22.4\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install numpy==1.22.4"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:51:59.678557Z","iopub.status.busy":"2023-11-13T17:51:59.678200Z","iopub.status.idle":"2023-11-13T17:51:59.688582Z","shell.execute_reply":"2023-11-13T17:51:59.687452Z","shell.execute_reply.started":"2023-11-13T17:51:59.678528Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import pandas as pd\n","from skimage import io, transform\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import torch.nn as nn\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","plt.ion()   # interactive mode"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preparation\n","- Create Dataloader class\n","\n","Note: Working on Part (a) as of now."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:34:51.523866Z","iopub.status.busy":"2023-11-13T17:34:51.523123Z","iopub.status.idle":"2023-11-13T17:34:51.532580Z","shell.execute_reply":"2023-11-13T17:34:51.531555Z","shell.execute_reply.started":"2023-11-13T17:34:51.523832Z"},"trusted":true},"outputs":[],"source":["START_TOKEN = \"START\"\n","END_TOKEN = \"END\"\n","UNK_TOKEN = \"UNK\"\n","\n","class Vocabulary:\n","    def __init__(self, freq_dict, wd_to_id, id_to_wd):\n","        self.freq_dict = freq_dict\n","        self.wd_to_id = wd_to_id\n","        self.id_to_wd = id_to_wd\n","        self.N = len(freq_dict)\n","    \n","class LatexFormulaDataset(Dataset):\n","    \"\"\"Latex Formula Dataset: Image and Text\"\"\"\n","    \n","    def __init__(self, csv_file, root_dir, transform = None):\n","        \"\"\"\n","        Arguments:\n","            csv_file (string): Path to the csv file with image name and text\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        #@TODO: May want to preload images\n","        self.df = pd.read_csv(csv_file)\n","        \n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","        '''Tokenize the formula by splitting on spaces'''\n","        self.df['formula'] = self.df['formula'].apply(lambda x: x.split())\n","        self.vocab= self.construct_vocab()  \n","\n","        maxlen = 0\n","        for formula in self.df['formula']:\n","            if len(formula) > maxlen:\n","                maxlen = len(formula)\n","\n","        self.df['formula'] = self.df['formula'].apply(lambda x: x + [UNK_TOKEN]*(maxlen - len(x)))\n","\n","        #Embedding layer\n","        self.embed = nn.Embedding(self.vocab.N, 512)\n","        \n","\n","\n","    def __len__(self):\n","        return len(self.landmarks_frame)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns sample of type image, textformula\n","        \"\"\"\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir,\n","                                self.df.iloc[idx, 0])\n","        image = io.imread(img_name)\n","        formula = self.df.iloc[idx, 1]\n","        formula = np.array([formula], dtype=str).reshape(-1, 1)\n","        sample = {'image': image, 'formula': formula}\n","\n","        if self.transform:\n","            sample['image'] = self.transform(sample['image'])\n","\n","        return sample\n","\n","    def construct_vocab(self):\n","        \"\"\"\n","        Constructs vocabulary from the dataset formulas\n","        \"\"\"\n","        freq_dict = {}\n","        for formula in self.df['formula']:\n","            for wd in formula:\n","                if wd not in freq_dict:\n","                    freq_dict[wd] = 1\n","                else:\n","                    freq_dict[wd] += 1\n","        freq_dict[START_TOKEN] = 1\n","        freq_dict[END_TOKEN] = 1\n","        freq_dict[UNK_TOKEN] = 1\n","        N = len(freq_dict)\n","        wd_to_id = {}\n","        for i, wd in enumerate(freq_dict):\n","            wd_to_id[wd] = i\n","        id_to_wd = {v: k for k, v in wd_to_id.items()}\n","    \n","        #pad the formulas with \n","        return Vocabulary(freq_dict, wd_to_id, id_to_wd)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:51:07.821400Z","iopub.status.busy":"2023-11-13T17:51:07.821028Z","iopub.status.idle":"2023-11-13T17:51:07.826658Z","shell.execute_reply":"2023-11-13T17:51:07.825542Z","shell.execute_reply.started":"2023-11-13T17:51:07.821371Z"},"trusted":true},"outputs":[],"source":["# image processing\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((224, 224)),\n","    transforms.Lambda(lambda x: x/255.0), #min-max normalisation\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:51:08.774305Z","iopub.status.busy":"2023-11-13T17:51:08.773240Z","iopub.status.idle":"2023-11-13T17:51:08.782964Z","shell.execute_reply":"2023-11-13T17:51:08.781920Z","shell.execute_reply.started":"2023-11-13T17:51:08.774267Z"},"trusted":true},"outputs":[],"source":["image = io.imread('/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/images/100009e256.png')\n","image = np.asarray(image)\n","image.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T17:51:12.844970Z","iopub.status.busy":"2023-11-13T17:51:12.844235Z","iopub.status.idle":"2023-11-13T17:51:13.020102Z","shell.execute_reply":"2023-11-13T17:51:13.019072Z","shell.execute_reply.started":"2023-11-13T17:51:12.844937Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["#part a\n","train_csv_path = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/train.csv\"\n","image_root_path = \"/kaggle/input/converting-handwritten-equations-to-latex-code/col_774_A4_2023/SyntheticData/images\"\n","train_set = LatexFormulaDataset(train_csv_path, image_root_path, transform)\n","\n","print(train_set[1]['image'])"]},{"cell_type":"markdown","metadata":{},"source":["### Encoder Network\n","- A CNN to encode image to more meaningful vector"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T18:15:50.390411Z","iopub.status.busy":"2023-11-13T18:15:50.390048Z","iopub.status.idle":"2023-11-13T18:15:50.401998Z","shell.execute_reply":"2023-11-13T18:15:50.401089Z","shell.execute_reply.started":"2023-11-13T18:15:50.390380Z"},"trusted":true},"outputs":[],"source":["class EncoderCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        self.conv1 = nn.Conv2d(3, 32, (5, 5))\n","        self.act1 = nn.ReLU()\n","        self.pool1 = nn.MaxPool2d((2,2))\n","        \n","        self.conv2 = nn.Conv2d(32, 64, (5, 5))\n","        self.act2 = nn.ReLU()\n","        self.pool2 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv3 = nn.Conv2d(64, 128, (5, 5))\n","        self.act3 = nn.ReLU()\n","        self.pool3 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv4 = nn.Conv2d(128, 256, (5, 5))\n","        self.act4 = nn.ReLU()\n","        self.pool4 = nn.MaxPool2d((2, 2))\n","        \n","        self.conv5 = nn.Conv2d(256, 512, (5, 5))\n","        self.act5 = nn.ReLU()\n","        self.pool5 = nn.MaxPool2d((2, 2))\n","        \n","        self.avg_pool = nn.AvgPool2d((3, 3))\n","    \n","    def forward(self, x):\n","        x = self.act1(self.conv1(x))\n","        x = self.pool1(x)\n","        \n","        x = self.act2(self.conv2(x))\n","        x = self.pool2(x)\n","        \n","        x = self.act3(self.conv3(x))\n","        x = self.pool3(x)\n","        \n","        x = self.act4(self.conv4(x))\n","        x = self.pool4(x)\n","        \n","        x = self.act5(self.conv5(x))\n","        x = self.pool5(x)\n","        \n","        x = self.avg_pool(x)\n","        x = x.reshape((1, 1, 512))\n","        return x"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-11-13T18:15:56.106481Z","iopub.status.busy":"2023-11-13T18:15:56.105734Z","iopub.status.idle":"2023-11-13T18:15:56.198416Z","shell.execute_reply":"2023-11-13T18:15:56.197693Z","shell.execute_reply.started":"2023-11-13T18:15:56.106446Z"}},"source":["### Vocabulary\n","- https://github.com/harvardnlp/im2markup/blob/master\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Decoder Network"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DecoderLSTM(nn.Module):\n","    def __init__(self, vocab_size, output_size):\n","        "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
